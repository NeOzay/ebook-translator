"""Helper centralis√© pour g√©rer les retries avec mode raisonnement."""

from typing import Optional, Callable
from ebook_translator.checks.base import ValidationContext
from ebook_translator.logger import get_logger

logger = get_logger(__name__)


def retry_with_reasoning(
    context: ValidationContext,
    render_prompt: Callable[[int, bool], str],
    validate_result: Callable[[str], bool],
    context_name: str,
    max_attempts: int = 2,
    llm_content: str = "",
) -> tuple[bool, Optional[str]]:
    """
    Ex√©cute une correction avec retry automatique (normal ‚Üí reasoning).

    Args:
        context: Contexte de validation
        chunk_index: Index du chunk (pour logs)
        render_prompt: Fonction qui g√©n√®re le prompt. Prend use_reasoning en param√®tre.
        validate_result: Fonction qui valide le r√©sultat LLM. Retourne True si valide.
        context_name: Nom du contexte pour logs (ex: "fragment", "missing_lines", "punctuation")
        max_attempts: Nombre maximum de tentatives (d√©faut: 2)

    Returns:
        Tuple (succ√®s: bool, r√©sultat_llm: str | None)
        - Si succ√®s: (True, r√©sultat_valide)
        - Si √©chec apr√®s tous les retries: (False, None)

    Flow:
        Tentative 1: Mode normal (deepseek-chat)
        Tentative 2+: Mode reasoning (deepseek-reasoner)
    """
    if context.llm is None:
        logger.warning(f"‚ö†Ô∏è LLM non disponible pour correction {context_name}")
        return False, None

    chunk_index: int = context.chunk.index
    for attempt in range(1, max_attempts + 1):
        # Tentative 1 : mode normal (deepseek-chat)
        # Tentative 2+ : mode reasoning (deepseek-reasoner)
        use_reasoning = attempt == max_attempts

        # G√©n√©rer le prompt (m√™me template, param√®tre use_reasoning pour contexte)
        prompt = render_prompt(attempt, use_reasoning)

        # Construire le contexte de log
        llm_context = (
            f"correction_{context_name}_chunk_{chunk_index:03d}_attempt_{attempt + 1}"
        )
        if use_reasoning:
            llm_context += "_reasoning"

        # Log de la tentative
        if use_reasoning:
            logger.info(
                f"üß† Tentative {attempt}/{max_attempts} avec mode raisonnement : {context_name}"
            )
        else:
            logger.info(
                f"üîÑ Tentative {attempt}/{max_attempts} mode normal : {context_name}"
            )

        # Appeler le LLM
        try:
            llm_output = context.llm.query(
                system_prompt=prompt,
                content=llm_content,
                context=llm_context,
                use_reasoning_mode=use_reasoning,
            )
        except Exception as e:
            logger.error(f"‚ùå Erreur LLM lors de la tentative {attempt} : {e}")
            continue

        # Valider le r√©sultat
        try:
            is_valid = validate_result(llm_output)
            if is_valid:
                logger.info(f"‚úÖ Correction r√©ussie apr√®s {attempt} tentative(s)")
                return True, llm_output
            else:
                logger.warning(
                    f"‚ö†Ô∏è Tentative {attempt} √©chou√©e, validation non satisfaite"
                )
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Tentative {attempt} √©chou√©e, erreur validation : {e}")

    # Toutes les tentatives ont √©chou√©
    logger.error(
        f"‚ùå √âchec de correction apr√®s {max_attempts} tentatives : {context_name}"
    )
    return False, None
