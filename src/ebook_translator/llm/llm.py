import os
import datetime
from pathlib import Path
import sys
import time
from dotenv import load_dotenv
from jinja2 import Environment, FileSystemLoader, select_autoescape
from typing import Optional, Callable, Awaitable
from openai import OpenAI, OpenAIError, APITimeoutError, RateLimitError, APIError
from openai.types.chat import ChatCompletionMessageParam

from ..logger import get_logger, get_session_log_path
from .template_renderers import TemplateRenderer

logger = get_logger(__name__)


def get_api_key() -> str:
    # Charger les variables d'environnement depuis .env
    load_dotenv()

    # Configuration du LLM avec validation
    api_key = os.getenv("API_KEY")
    if not api_key:
        print("\n‚ùå ERREUR : La cl√© API DeepSeek n'est pas d√©finie.", file=sys.stderr)
        print("\nPour configurer :", file=sys.stderr)
        print("  1. Copiez .env.example en .env", file=sys.stderr)
        print(
            "  2. Obtenez une cl√© API sur https://platform.deepseek.com/api_keys",
            file=sys.stderr,
        )
        print(
            "  3. Ajoutez votre cl√© dans .env : DEEPSEEK_API_KEY=sk-votre-cle",
            file=sys.stderr,
        )
        print(
            "\nDocumentation : voir CLAUDE.md section 'Configuration des cl√©s API'\n",
            file=sys.stderr,
        )
        sys.exit(1)
    return api_key


class LLM:
    """
    Classe asynchrone pour g√©rer un LLM (DeepSeek, GPT, etc.)
    avec :
      - rendu de templates Jinja2,
      - logs cr√©√©s √† l‚Äôenvoi,
      - callback ex√©cut√© √† la r√©ception,
      - ex√©cution parall√®le avec limite de simultan√©it√©.
    """

    def __init__(
        self,
        model_name: str,
        url: str,
        api_key: Optional[str] = None,
        prompt_dir: str = "template",
        temperature: float = 0.5,
        max_retries: int = 3,
        retry_delay: float = 1.0,
    ):
        self.model_name = model_name
        self.api_key = api_key or get_api_key()
        self.client = OpenAI(api_key=self.api_key, base_url=url)
        self.temperature = temperature
        self.max_tokens = 4000
        self.max_retries = max_retries
        self.retry_delay = retry_delay

        # Compteur pour nommage unique des logs
        self._log_counter = 0

        # Renderer encapsul√© pour templates typ√©s (API recommand√©e)
        self.renderer = TemplateRenderer(prompt_dir)

    # -----------------------------------
    # üîπ Rendu du template
    # -----------------------------------
    def render_prompt(self, template_name: str, **kwargs) -> str:
        """
        Rend un template Jinja2 avec les variables donn√©es.

        Note:
            Cette m√©thode est conserv√©e pour r√©trocompatibilit√©.
            Pour une API typ√©e et simplifi√©e, utilisez `self.renderer.render_XXX()`.

        Example:
            >>> # API recommand√©e (typ√©e)
            >>> prompt = llm.renderer.render_translate(target_language="fr")
            >>>
            >>> # API legacy (non typ√©e, conserv√©e pour compatibilit√©)
            >>> prompt = llm.render_prompt("translate.jinja", target_language="fr")

        Args:
            template_name: Nom du fichier template (ex: "translate.jinja")
            **kwargs: Variables √† passer au template

        Returns:
            Prompt rendu
        """
        return self.renderer.render_prompt(template_name, **kwargs)

    # -----------------------------------
    # üîπ Gestion du log
    # -----------------------------------
    def _create_log(
        self, prompt: str, content: str, context: Optional[str] = None
    ) -> Path:
        """
        Pr√©pare les donn√©es du log et retourne le chemin du fichier.

        Le fichier ne sera cr√©√© qu'au moment de l'ajout de la r√©ponse (lazy).

        Args:
            prompt: Le prompt syst√®me envoy√© au LLM
            content: Le contenu √† traiter
            context: Contexte optionnel pour nommer le fichier (ex: "chunk_042", "retry_phase1")

        Returns:
            Chemin du fichier de log (non encore cr√©√©)
        """

        timestamp = datetime.datetime.now().isoformat().replace(":", "-")

        # G√©n√©rer un nom de fichier contextuel
        self._log_counter += 1
        if context:
            # Format : llm_<context>_<counter>.log
            filename = f"llm_{context}_{self._log_counter:04d}_{timestamp}.log"
        else:
            # Format par d√©faut : llm_<counter>.log
            filename = f"llm_{self._log_counter:04d}_{timestamp}.log"

        log_path = get_session_log_path(filename)

        header = (
            f"=== LLM REQUEST LOG ===\n"
            f"Timestamp : {timestamp}\n"
            f"Model     : {self.model_name}\n"
            f"Prompt len: {len(prompt)} chars\n"
            f"{'-'*40}\n\n"
            f"--- PROMPT ---\n{prompt}\n\n"
            f"--- CONTENT ---\n{content}\n\n"
            f"--- RESPONSE ---\n"
        )
        with open(log_path, "w", encoding="utf-8") as f:
            f.write(header)
        return log_path

    def _append_response(self, log_path: Path, response: str):
        """Ajoute la r√©ponse √† la fin du log existant."""
        with open(log_path, "a", encoding="utf-8") as f:
            f.write(response.strip() + "\n")

    # -----------------------------------
    # üîπ Requ√™te asynchrone simple
    # -----------------------------------
    def query(
        self,
        system_prompt: str,
        content: str,
        context: Optional[str] = None,
        use_reasoning_mode: bool = False,
    ) -> str:
        """
        Envoie une requ√™te au LLM avec gestion d'erreurs sp√©cifiques et retry automatique.

        Args:
            system_prompt: Le prompt syst√®me d√©finissant le comportement du LLM
            content: Le contenu √† traiter
            context: Contexte optionnel pour nommer le fichier de log
                    (ex: "chunk_042", "retry_phase1", "validation")
            use_reasoning_mode: Si True, utilise deepseek-reasoner au lieu de deepseek-chat.
                               Le mod√®le g√©n√®re alors un reasoning_content explicite.

        Returns:
            La r√©ponse du LLM ou un message d'erreur entre crochets

        Note:
            Les erreurs sont logg√©es et un fichier de log est cr√©√© pour chaque requ√™te.
            Les erreurs Timeout et RateLimitError d√©clenchent un retry automatique
            avec backoff exponentiel.
            Le fichier de log n'est cr√©√© qu'au moment o√π la r√©ponse est disponible.

            En mode raisonnement (use_reasoning_mode=True), le mod√®le deepseek-reasoner
            g√©n√®re un processus de pens√©e explicite (reasoning_content) qui est logg√©
            s√©par√©ment pour faciliter le debugging des corrections complexes.
        """
        log_path = self._create_log(system_prompt, content, context)
        last_error: Optional[Exception] = None

        # Choisir le mod√®le selon le mode
        model_name = "deepseek-reasoner" if use_reasoning_mode else self.model_name

        # Log du mode utilis√©
        if use_reasoning_mode:
            logger.info(f"üß† Mode raisonnement activ√© pour : {context}")

        for attempt in range(self.max_retries):
            try:
                messages: list[ChatCompletionMessageParam] = [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": content},
                ]
                resp = self.client.chat.completions.create(
                    model=model_name,
                    messages=messages,
                    temperature=self.temperature,
                    max_tokens=self.max_tokens,
                )
                result = resp.choices[0].message.content
                response_text = result.strip() if result is not None else "Result Empty"

                # Extraire le raisonnement si pr√©sent (deepseek-reasoner)
                reasoning_text = ""
                if hasattr(resp.choices[0].message, "reasoning_content"):
                    reasoning_content = resp.choices[0].message.reasoning_content  # type: ignore
                    reasoning_text = (
                        reasoning_content if reasoning_content is not None else ""
                    )

                if attempt > 0:
                    logger.info(
                        f"‚úÖ Requ√™te LLM r√©ussie apr√®s {attempt + 1} tentative(s) "
                        f"({len(content)} chars)"
                    )
                else:
                    logger.info(f"‚úÖ Requ√™te LLM r√©ussie ({len(content)} chars)")

                # Logger avec raisonnement s√©par√© si pr√©sent
                if reasoning_text:
                    log_content = f"""
{'=' * 80}
üß† REASONING (deepseek-reasoner):
{'=' * 80}
{reasoning_text}

{'=' * 80}
üìù RESPONSE:
{'=' * 80}
{response_text}
"""
                    self._append_response(log_path, log_content)
                else:
                    self._append_response(log_path, response_text)

                return response_text

            except APITimeoutError as e:
                last_error = e
                logger.warning(
                    f"‚è±Ô∏è Timeout API (tentative {attempt + 1}/{self.max_retries}): {e}"
                )
                if attempt < self.max_retries - 1:
                    delay = self.retry_delay * (2**attempt)
                    logger.info(
                        f"‚è≥ Attente de {delay:.1f}s avant nouvelle tentative..."
                    )
                    time.sleep(delay)
                    continue

            except RateLimitError as e:
                last_error = e
                logger.warning(
                    f"üö¶ Limite de d√©bit atteinte (tentative {attempt + 1}/{self.max_retries}): {e}"
                )
                if attempt < self.max_retries - 1:
                    # Pour rate limit, attendre plus longtemps
                    delay = self.retry_delay * (3**attempt)
                    logger.info(
                        f"‚è≥ Attente de {delay:.1f}s avant nouvelle tentative..."
                    )
                    time.sleep(delay)
                    continue

            except APIError as e:
                # Les erreurs API ne sont g√©n√©ralement pas r√©cup√©rables par retry
                logger.error(f"‚ùå Erreur API: {e}")
                response_text = f"[ERREUR API: {e}]"
                self._append_response(log_path, response_text)
                return response_text

            except OpenAIError as e:
                logger.error(f"‚ùå Erreur OpenAI g√©n√©rique: {e}")
                response_text = f"[ERREUR OPENAI: {e}]"
                self._append_response(log_path, response_text)
                return response_text

            except Exception as e:
                logger.exception(f"‚ùå Erreur inattendue lors de la requ√™te LLM: {e}")
                response_text = f"[ERREUR INCONNUE: {e}]"
                self._append_response(log_path, response_text)
                return response_text

        # Si on arrive ici, tous les retries ont √©chou√©
        if isinstance(last_error, APITimeoutError):
            response_text = (
                f"[ERREUR: Timeout apr√®s {self.max_retries} tentatives - "
                f"Le serveur n'a pas r√©pondu √† temps]"
            )
        elif isinstance(last_error, RateLimitError):
            response_text = (
                f"[ERREUR: Rate limit apr√®s {self.max_retries} tentatives - "
                f"Trop de requ√™tes, veuillez patienter]"
            )
        else:
            response_text = f"[ERREUR: √âchec apr√®s {self.max_retries} tentatives]"

        logger.error(f"‚ùå √âchec d√©finitif apr√®s {self.max_retries} tentatives")
        self._append_response(log_path, response_text)
        return response_text
