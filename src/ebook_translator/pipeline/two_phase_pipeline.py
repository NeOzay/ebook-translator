"""
Orchestrateur principal du pipeline de traduction en 2 phases.

Ce module g√®re le workflow complet :
- Phase 1 : Traduction initiale (gros blocs 1500 tokens, parall√®le)
- Phase 2 : Affinage avec glossaire (petits blocs 300 tokens, s√©quentiel)
- Thread de correction d√©di√© (erreurs asynchrones)
- Reconstruction EPUB avec fallback refined ‚Üí initial
"""

from pathlib import Path
from typing import TYPE_CHECKING

from ebooklib import epub

from ..logger import get_logger
from ..stores.multi_store import MultiStore
from ..glossary import Glossary
from ..correction.error_queue import ErrorQueue
from ..correction.correction_worker_pool import CorrectionWorkerPool
from ..segment import Segmentator
from ..translation.epub_handler import (
    copy_epub_metadata,
    extract_html_items_in_spine_order,
    reconstruct_html_item,
)
from .phase1_worker import Phase1Worker
from .phase2_worker import Phase2Worker
from .glossary_validator import GlossaryValidator

if TYPE_CHECKING:
    from ..llm import LLM
    from ..translation.translator import Language

logger = get_logger(__name__)


class TwoPhasePipeline:
    """
    Pipeline de traduction EPUB en 2 phases avec affinage.

    Architecture :
    1. Phase 1 (parall√®le) :
       - Segmentation 1500 tokens
       - Traduction initiale
       - Apprentissage glossaire automatique
       - Sauvegarde dans initial_store

    2. Transition :
       - Switch stores (initial ‚Üí refined)
       - Export glossaire

    3. Phase 2 (s√©quentiel) :
       - Segmentation 300 tokens
       - Affinage avec glossaire + traduction initiale
       - Sauvegarde dans refined_store

    4. Correction asynchrone :
       - Thread d√©di√© consommant ErrorQueue
       - Retry automatique jusqu'√† max_retries
       - Erreurs non-bloquantes

    5. Reconstruction EPUB :
       - Fallback refined ‚Üí initial ‚Üí original
       - M√©tadonn√©es pr√©serv√©es

    Attributes:
        llm: Instance LLM pour traduction et affinage
        epub_path: Chemin vers l'EPUB source
        cache_dir: R√©pertoire racine des caches
        multi_store: Gestionnaire initial_store + refined_store
        glossary: Glossary unifi√© pour coh√©rence
        error_queue: Queue thread-safe pour erreurs
        correction_worker: Thread daemon de correction

    Example:
        >>> pipeline = TwoPhasePipeline(llm, "book.epub", Path("cache"))
        >>> stats = pipeline.run(Language.FRENCH, "book_fr.epub", phase1_workers=4)
        >>> print(f"Phase 1: {stats['phase1']['translated']}")
        >>> print(f"Phase 2: {stats['phase2']['refined']}")
        >>> print(f"Glossaire: {stats['glossary']['total_terms']} termes")
    """

    def __init__(
        self,
        llm: "LLM",
        epub_path: str | Path,
        cache_dir: str | Path | None = None,
    ):
        """
        Initialise le pipeline en 2 phases.

        Args:
            llm: Instance LLM pour traduction et affinage
            epub_path: Chemin vers l'EPUB source
            cache_dir: R√©pertoire pour caches (initial/, refined/, glossary.json)
        """
        self.llm = llm
        self.epub_path = epub_path if isinstance(epub_path, Path) else Path(epub_path)
        match cache_dir:
            case None:
                cache_dir = Path(
                    self.epub_path.parent / f".{self.epub_path.stem}_cache"
                )
            case str():
                cache_dir = Path(cache_dir)
        self.cache_dir = cache_dir

        # Valider que l'EPUB existe
        if not self.epub_path.exists():
            raise FileNotFoundError(f"EPUB source introuvable : {self.epub_path}")

        # Cr√©er cache_dir si n√©cessaire
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        # Initialiser infrastructure
        self.multi_store = MultiStore(self.cache_dir)
        self.glossary = Glossary(cache_path=self.cache_dir / "glossary.json")
        self.error_queue = ErrorQueue(maxsize=100)
        self.correction_worker_pool: CorrectionWorkerPool | None = None

        # Statistiques globales
        self.phase1_stats: dict = {}
        self.phase2_stats: dict = {}
        self.correction_stats: dict = {}

    def run(
        self,
        target_language: "Language | str",
        output_epub: str | Path,
        phase1_workers: int = 4,
        phase1_max_tokens: int = 1500,
        phase2_max_tokens: int = 300,
        correction_workers: int = 2,
        correction_timeout: float = 30.0,
        auto_validate_glossary: bool = False,
    ) -> dict:
        """
        Ex√©cute le pipeline complet de traduction en 2 phases.

        Args:
            target_language: Langue cible (enum Language ou str)
            output_epub: Chemin de sortie pour l'EPUB traduit
            phase1_workers: Nombre de threads parall√®les Phase 1 (d√©faut: 4)
            phase1_max_tokens: Taille max chunks Phase 1 (d√©faut: 1500)
            phase2_max_tokens: Taille max chunks Phase 2 (d√©faut: 300)
            correction_workers: Nombre de threads parall√®les pour corrections (d√©faut: 2)
            correction_timeout: Timeout pour arr√™t CorrectionWorkerPool (d√©faut: 30s)
            auto_validate_glossary: Si True, r√©sout automatiquement les conflits
                                   sans demander validation utilisateur (d√©faut: False)

        Returns:
            Statistiques compl√®tes :
            {
                "phase1": {...},
                "phase2": {...},
                "corrections": {...},
                "glossary": {...},
                "total_duration": float
            }

        Example:
            >>> stats = pipeline.run(
            ...     target_language=Language.FRENCH,
            ...     output_epub="book_fr.epub",
            ...     phase1_workers=4,
            ... )
        """
        import time

        start_time = time.time()

        # Normaliser target_language
        from ..translation.translator import Language

        target_language_str = (
            target_language.value
            if isinstance(target_language, Language)
            else target_language
        )

        output_epub = (
            output_epub if isinstance(output_epub, Path) else Path(output_epub)
        )

        logger.info(
            f"üöÄ D√©marrage pipeline 2 phases : {self.epub_path} ‚Üí {output_epub}"
        )
        logger.info(f"  ‚Ä¢ Langue cible: {target_language_str}")
        logger.info(
            f"  ‚Ä¢ Phase 1: {phase1_max_tokens} tokens, {phase1_workers} workers"
        )
        logger.info(f"  ‚Ä¢ Phase 2: {phase2_max_tokens} tokens, s√©quentiel")
        logger.info(f"  ‚Ä¢ Corrections: {correction_workers} workers parall√®les")

        # =====================================================================
        # CHARGEMENT EPUB
        # =====================================================================
        logger.info("üìñ Chargement de l'EPUB source...")
        source_book = epub.read_epub(self.epub_path)
        html_items, target_book = extract_html_items_in_spine_order(source_book)
        copy_epub_metadata(source_book, target_book, str(target_language_str))
        logger.info(f"  ‚Ä¢ {len(html_items)} chapitres extraits")

        # =====================================================================
        # D√âMARRAGE CORRECTION WORKER POOL
        # =====================================================================
        logger.info("üîß D√©marrage du pool de correction...")
        self.correction_worker_pool = CorrectionWorkerPool(
            error_queue=self.error_queue,
            llm=self.llm,
            store=self.multi_store.initial_store,  # Commence avec initial
            target_language=target_language_str,
            num_workers=correction_workers,
        )
        self.correction_worker_pool.start()

        try:
            # =================================================================
            # PHASE 1 : TRADUCTION INITIALE (PARALL√àLE)
            # =================================================================
            logger.info("=" * 60)
            logger.info("üìù PHASE 1 : TRADUCTION INITIALE")
            logger.info("=" * 60)

            # Segmentation Phase 1 (gros blocs)
            segmentator_phase1 = Segmentator(html_items, max_tokens=phase1_max_tokens)
            chunks_phase1 = list(segmentator_phase1.get_all_segments())
            logger.info(
                f"  ‚Ä¢ {len(chunks_phase1)} chunks cr√©√©s ({phase1_max_tokens} tokens)"
            )

            # Worker Phase 1
            phase1_worker = Phase1Worker(
                llm=self.llm,
                store=self.multi_store.initial_store,
                glossary=self.glossary,
                error_queue=self.error_queue,
                target_language=target_language_str,
            )

            # Ex√©cuter Phase 1
            self.phase1_stats = phase1_worker.run_parallel(
                chunks=chunks_phase1,
                max_workers=phase1_workers,
            )

            # Statistiques glossaire apr√®s Phase 1
            glossary_stats = self.glossary.get_statistics()
            logger.info(f"üìö Glossaire appris: {glossary_stats['total_terms']} termes")

            # =================================================================
            # TRANSITION PHASE 1 ‚Üí PHASE 2
            # =================================================================
            logger.info("=" * 60)
            logger.info("üîÑ TRANSITION PHASE 1 ‚Üí PHASE 2")
            logger.info("=" * 60)

            # 1. V√©rifier que la queue d'erreurs est vide
            error_stats = self.error_queue.get_statistics()
            if error_stats.pending > 0:
                logger.warning(
                    f"‚ö†Ô∏è  {error_stats.pending} erreur(s) en attente de correction"
                )
                logger.info("‚è≥ Attente de fin des corrections avant Phase 2...")

                # Attendre que le CorrectionWorker finisse
                import time

                timeout = 60.0  # 60 secondes max
                start_wait = time.time()

                while error_stats.pending > 0 and (time.time() - start_wait) < timeout:
                    time.sleep(2.0)
                    error_stats = self.error_queue.get_statistics()
                    logger.info(f"  ‚Ä¢ Corrections restantes: {error_stats.pending}")

                # V√©rifier √† nouveau apr√®s attente
                error_stats = self.error_queue.get_statistics()
                if error_stats.pending > 0:
                    raise RuntimeError(
                        f"‚ùå Impossible de passer √† la Phase 2: {error_stats.pending} erreur(s) non corrig√©e(s)\n"
                        f"  ‚Ä¢ Corrig√©es: {error_stats.corrected}\n"
                        f"  ‚Ä¢ √âchou√©es: {error_stats.failed}\n"
                        f"  ‚Ä¢ En attente: {error_stats.pending}\n"
                        "Veuillez v√©rifier les logs pour plus de d√©tails."
                    )

            logger.info(
                "‚úÖ Queue d'erreurs vide, toutes les corrections sont termin√©es"
            )

            # 2. Validation du glossaire
            logger.info("\nüìö Validation du glossaire avant Phase 2...")
            validator = GlossaryValidator(self.glossary)

            glossary_validated = validator.validate_interactive(
                auto_resolve=auto_validate_glossary
            )

            if not glossary_validated:
                raise RuntimeError(
                    "‚ùå Validation du glossaire annul√©e par l'utilisateur.\n"
                    "La Phase 2 ne peut pas d√©marrer sans un glossaire valid√©."
                )

            logger.info(
                "‚úÖ Glossaire valid√©"
                + (
                    " automatiquement"
                    if auto_validate_glossary
                    else " par l'utilisateur"
                )
            )

            # Switch store pour refined
            self.multi_store.switch_to_refined()
            logger.info("  ‚Ä¢ MultiStore bascul√© vers refined_store")

            # Switch CorrectionWorkerPool vers refined_store
            self.correction_worker_pool.switch_all_stores(self.multi_store.refined_store)
            logger.info("  ‚Ä¢ CorrectionWorkerPool bascul√© vers refined_store")

            # Sauvegarder glossaire
            self.glossary.save()
            logger.info(f"  ‚Ä¢ Glossaire sauvegard√©: {self.cache_dir / 'glossary.json'}")

            # =================================================================
            # PHASE 2 : AFFINAGE AVEC GLOSSAIRE (S√âQUENTIEL)
            # =================================================================
            logger.info("=" * 60)
            logger.info("üé® PHASE 2 : AFFINAGE AVEC GLOSSAIRE")
            logger.info("=" * 60)

            # Segmentation Phase 2 (petits blocs)
            segmentator_phase2 = Segmentator(html_items, max_tokens=phase2_max_tokens)
            chunks_phase2 = list(segmentator_phase2.get_all_segments())
            logger.info(
                f"  ‚Ä¢ {len(chunks_phase2)} chunks cr√©√©s ({phase2_max_tokens} tokens)"
            )

            # Worker Phase 2
            phase2_worker = Phase2Worker(
                llm=self.llm,
                multi_store=self.multi_store,
                glossary=self.glossary,
                error_queue=self.error_queue,
                target_language=target_language_str,
            )

            # Ex√©cuter Phase 2
            self.phase2_stats = phase2_worker.run_sequential(chunks=chunks_phase2)

            # =================================================================
            # FINALISATION CORRECTIONS
            # =================================================================
            logger.info("=" * 60)
            logger.info("üõë FINALISATION DES CORRECTIONS")
            logger.info("=" * 60)

            # Arr√™ter CorrectionWorkerPool proprement
            logger.info(
                f"  ‚Ä¢ Attente de fin des corrections (timeout: {correction_timeout}s)..."
            )
            stopped = self.correction_worker_pool.stop(timeout=correction_timeout)

            if not stopped:
                logger.warning(
                    f"‚ö†Ô∏è CorrectionWorkerPool n'a pas pu s'arr√™ter dans le d√©lai ({correction_timeout}s)"
                )

            # Statistiques corrections (agr√©g√©es de tous les workers)
            self.correction_stats = {
                **self.correction_worker_pool.get_aggregated_statistics(),
                **self.error_queue.get_statistics().__dict__,
            }

            logger.info(
                f"  ‚Ä¢ Corrections r√©ussies: {self.correction_stats['corrected']}\n"
                f"  ‚Ä¢ Corrections √©chou√©es: {self.correction_stats['failed']}\n"
                f"  ‚Ä¢ Erreurs en attente: {self.correction_stats['pending']}"
            )

            # =================================================================
            # RECONSTRUCTION EPUB
            # =================================================================
            logger.info("=" * 60)
            logger.info("üî® RECONSTRUCTION EPUB")
            logger.info("=" * 60)

            logger.info("  ‚Ä¢ Reconstruction des pages HTML...")
            for item in html_items:
                reconstruct_html_item(item)
                target_book.add_item(item)

            # Sauvegarder EPUB traduit
            logger.info(f"  ‚Ä¢ Sauvegarde EPUB traduit: {output_epub}")
            if not output_epub.parent.exists():
                output_epub.parent.mkdir(parents=True, exist_ok=True)

            epub.write_epub(output_epub, target_book)

            # =================================================================
            # STATISTIQUES FINALES
            # =================================================================
            duration = time.time() - start_time

            stats = {
                "phase1": self.phase1_stats,
                "phase2": self.phase2_stats,
                "corrections": self.correction_stats,
                "glossary": glossary_stats,
                "total_duration": duration,
            }

            logger.info("=" * 60)
            logger.info("‚úÖ PIPELINE TERMIN√â")
            logger.info("=" * 60)
            logger.info(
                f"üìä R√âSUM√â:\n"
                f"  ‚Ä¢ Phase 1: {self.phase1_stats['translated']}/{self.phase1_stats['total_chunks']} chunks traduits\n"
                f"  ‚Ä¢ Phase 2: {self.phase2_stats['refined']}/{self.phase2_stats['total_chunks']} chunks affin√©s\n"
                f"  ‚Ä¢ Corrections: {self.correction_stats['corrected']} r√©ussies, {self.correction_stats['failed']} √©chou√©es\n"
                f"  ‚Ä¢ Glossaire: {glossary_stats['total_terms']} termes, {glossary_stats['validated_terms']} valid√©s\n"
                f"  ‚Ä¢ Dur√©e totale: {duration:.1f}s\n"
                f"  ‚Ä¢ EPUB final: {output_epub}"
            )

            return stats

        except KeyboardInterrupt:
            logger.error("‚ùå Pipeline interrompu par l'utilisateur")
            if self.correction_worker_pool:
                self.correction_worker_pool.stop(timeout=5.0)
            raise

        except Exception as e:
            logger.exception(f"‚ùå Erreur fatale dans le pipeline: {e}")
            if self.correction_worker_pool:
                self.correction_worker_pool.stop(timeout=5.0)
            raise

    def get_failed_errors(self) -> list:
        """
        R√©cup√®re la liste des erreurs non r√©cup√©rables.

        Returns:
            Liste des ErrorItem qui ont √©chou√© apr√®s tous les retries

        Example:
            >>> failed = pipeline.get_failed_errors()
            >>> for error in failed:
            ...     print(f"Chunk {error.chunk.index}: {error.error_type}")
        """
        return self.error_queue.get_failed_items()

    def clear_caches(self) -> None:
        """
        Supprime tous les caches (initial, refined, glossaire).

        Attention: Op√©ration irr√©versible.

        Example:
            >>> pipeline.clear_caches()
        """
        logger.warning("üóëÔ∏è Suppression de tous les caches...")
        self.multi_store.clear_all()
        glossary_path = self.cache_dir / "glossary.json"
        if glossary_path.exists():
            glossary_path.unlink()
        logger.info("‚úÖ Caches supprim√©s")
