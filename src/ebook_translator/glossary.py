"""
Syst√®me de glossaire unifi√© pour coh√©rence terminologique.

Ce module fournit un glossaire qui combine :
- Apprentissage automatique depuis paires texte/traduction
- Validation post-traduction et d√©tection de conflits
- Export format√© pour injection dans prompts LLM
- Persistance JSON avec reload automatique

Utilis√© √† la fois pour :
- Phase 1 : Apprentissage automatique des traductions
- Phase 2 : Injection du glossaire dans le prompt d'affinage
- Validation : D√©tection d'incoh√©rences terminologiques
"""

import json
import re
from pathlib import Path
from typing import Optional
from collections import defaultdict


class Glossary:
    """
    Glossaire unifi√© pour coh√©rence terminologique.

    G√®re l'apprentissage automatique, la validation et l'export de traductions
    de noms propres et termes techniques pour garantir la coh√©rence.

    Features:
    - learn(): Apprentissage terme par terme
    - learn_pair(): Apprentissage depuis textes complets avec extraction auto
    - get_translation(): R√©cup√©ration avec seuil de confiance
    - export_for_prompt(): Export format√© pour prompts LLM
    - get_conflicts(): D√©tection d'incoh√©rences
    - Persistance JSON automatique

    Example:
        >>> glossary = Glossary(Path("cache/glossary.json"))
        >>> # Apprentissage basique
        >>> glossary.learn("Matrix", "Matrice")
        >>> # Apprentissage depuis paire
        >>> glossary.learn_pair(
        ...     "Dr. Sakamoto activated the Matrix.",
        ...     "Le Dr Sakamoto activa la Matrice."
        ... )
        >>> # Export pour prompt
        >>> prompt_glossary = glossary.export_for_prompt()
        >>> # 'Matrix ‚Üí Matrice, Sakamoto ‚Üí Sakamoto'
    """

    def __init__(self, cache_path: Optional[Path] = None):
        """
        Initialise le glossaire.

        Args:
            cache_path: Chemin optionnel pour sauvegarder/charger le glossaire
        """
        self.cache_path = cache_path
        # {terme_source: {traduction: count}}
        self._glossary: dict[str, dict[str, int]] = defaultdict(
            lambda: defaultdict(int)
        )
        # {terme_source: traduction_valid√©e}
        self._validated: dict[str, str] = {}

        if cache_path and cache_path.exists():
            self._load_from_cache()

    # =========================================================================
    # API basique : Apprentissage et r√©cup√©ration
    # =========================================================================

    def learn(self, source_term: str, translated_term: str) -> None:
        """
        Enregistre une traduction observ√©e.

        Filtre automatiquement les mots grammaticaux et mots tr√®s courts
        pour √©viter la pollution du glossaire.

        Args:
            source_term: Terme dans la langue source
            translated_term: Traduction observ√©e

        Example:
            >>> glossary.learn("Matrix", "Matrice")
            >>> glossary.learn("Matrix", "Matrice")  # Renforce
            >>> glossary.learn("Matrix", "Syst√®me")  # Conflit d√©tect√©
            >>> glossary.learn("the", "le")  # Ignor√© (stopword)
        """
        from .glossary_filters import should_exclude_from_glossary

        # Filtrer mots grammaticaux et mots courts automatiquement
        if should_exclude_from_glossary(source_term):
            return  # Ignorer silencieusement

        # Ignorer si identiques (ex: noms propres gard√©s tels quels)
        # if source_term == translated_term:
        #     return

        # Incr√©menter le compteur
        self._glossary[source_term][translated_term] += 1

    def get_translation(
        self,
        source_term: str,
        min_confidence: float = 0.5,
    ) -> Optional[str]:
        """
        R√©cup√®re la traduction recommand√©e d'un terme.

        Retourne la traduction valid√©e, ou √† d√©faut la plus fr√©quente
        si elle d√©passe le seuil de confiance.

        Args:
            source_term: Le terme √† traduire
            min_confidence: Seuil de confiance minimum (ratio de la traduction dominante)

        Returns:
            Traduction recommand√©e, ou None si aucune traduction fiable

        Example:
            >>> glossary.get_translation("Matrix")
            'Matrice'
            >>> glossary.get_translation("UnknownTerm")
            None
        """
        # 1. V√©rifier s'il y a une traduction valid√©e manuellement
        if source_term in self._validated:
            return self._validated[source_term]

        # 2. Sinon, chercher dans les traductions apprises
        if source_term not in self._glossary:
            return None

        translations = self._glossary[source_term]
        if not translations:
            return None

        # Trouver la traduction la plus fr√©quente
        total = sum(translations.values())
        most_frequent = max(translations, key=translations.get)  # type: ignore
        frequency = translations[most_frequent]
        confidence = frequency / total

        # V√©rifier le seuil de confiance
        if confidence >= min_confidence:
            return most_frequent

        return None

    def validate_translation(
        self,
        source_term: str,
        validated_translation: str,
    ) -> None:
        """
        Valide manuellement une traduction.

        Les traductions valid√©es ont priorit√© sur celles apprises automatiquement.

        Args:
            source_term: Terme source
            validated_translation: Traduction valid√©e

        Example:
            >>> glossary.validate_translation("Matrix", "Matrice")
            >>> # Cette traduction sera toujours utilis√©e
        """
        self._validated[source_term] = validated_translation

    # =========================================================================
    # API avanc√©e : Apprentissage depuis paires compl√®tes
    # =========================================================================

    def learn_pair(self, original_text: str, translated_text: str) -> None:
        """
        Apprend depuis une paire (texte_original, texte_traduit).

        Extrait automatiquement les noms propres et termes techniques,
        puis les aligne pour apprendre les traductions.

        Args:
            original_text: Texte dans la langue source
            translated_text: Texte traduit

        Example:
            >>> glossary.learn_pair(
            ...     "Dr. Sakamoto activated the Temporal Matrix.",
            ...     "Le Dr Sakamoto activa la Matrice Temporelle."
            ... )
            >>> # Apprend: Sakamoto ‚Üí Sakamoto, Matrix ‚Üí Matrice
        """
        # Extraire les termes des deux textes
        original_terms = self._extract_terms(original_text)
        translated_terms = self._extract_terms(translated_text)

        # Alignement simple : chercher les correspondances
        for original_term in original_terms:
            # Chercher le terme original dans le texte traduit
            # (peut √™tre gard√© tel quel, ex: noms propres)
            if original_term in translated_text:
                # Terme gard√© identique
                self.learn(original_term, original_term)
            else:
                # Chercher la meilleure correspondance par similarit√©
                best_match = self._find_best_match(
                    original_term, translated_terms, original_text, translated_text
                )
                if best_match:
                    self.learn(original_term, best_match)

    def _extract_terms(self, text: str) -> list[str]:
        """
        Extrait les noms propres et termes techniques d'un texte.

        Crit√®res d'extraction:
        - Mots commen√ßant par une majuscule (hors d√©but de phrase)
        - Acronymes (2+ majuscules cons√©cutives)
        - Termes techniques (CamelCase)

        Args:
            text: Texte √† analyser

        Returns:
            Liste de termes uniques extraits

        Example:
            >>> glossary._extract_terms("Dr. Sakamoto used the DNA Matrix.")
            ['Sakamoto', 'DNA', 'Matrix']
        """
        terms: set[str] = set()

        # Pattern 1: Mots avec majuscule (mais pas d√©but de phrase)
        # Exclure les titres communs (Dr., Mr., Mrs., etc.)
        words = text.split()
        for i, word in enumerate(words):
            # Nettoyer la ponctuation
            clean_word = re.sub(r"[^\w]", "", word)

            # Skip mots vides ou trop courts
            if len(clean_word) < 2:
                continue

            # Skip titres communs
            if clean_word in ["Dr", "Mr", "Mrs", "Ms", "Prof", "Sir", "Lady"]:
                continue

            # Chercher mots avec majuscule
            if clean_word and clean_word[0].isupper():
                # Si c'est le premier mot, v√©rifier qu'il soit suivi d'un autre mot capitalis√©
                if i == 0:
                    if (
                        i + 1 < len(words)
                        and words[i + 1]
                        and words[i + 1][0].isupper()
                    ):
                        terms.add(clean_word)
                else:
                    # Pas le premier mot ‚Üí ajouter
                    terms.add(clean_word)

        # Pattern 2: Acronymes (2+ majuscules cons√©cutives)
        acronyms = re.findall(r"\b[A-Z]{2,}\b", text)
        terms.update(acronyms)

        # Pattern 3: Termes techniques (ex: CamelCase)
        camel_case = re.findall(r"\b[A-Z][a-z]+[A-Z][a-zA-Z]*\b", text)
        terms.update(camel_case)

        return sorted(list(terms))

    def _find_best_match(
        self,
        original_term: str,
        candidate_terms: list[str],
        original_text: str,
        translated_text: str,
    ) -> Optional[str]:
        """
        Trouve la meilleure correspondance pour un terme original.

        Utilise plusieurs heuristiques:
        1. Position relative dans le texte (poids 2x)
        2. Similarit√© de longueur
        3. Similarit√© phon√©tique (premi√®res lettres)

        Args:
            original_term: Terme √† traduire
            candidate_terms: Termes candidats dans la traduction
            original_text: Texte original complet
            translated_text: Texte traduit complet

        Returns:
            Meilleur terme correspondant ou None (seuil minimum 1.0)
        """
        if not candidate_terms:
            return None

        # Calculer la position relative du terme original (0.0 √† 1.0)
        original_pos = original_text.find(original_term)
        if original_pos == -1:
            return None

        original_rel_pos = original_pos / len(original_text)

        # Scorer chaque candidat
        scores: list[tuple[str, float]] = []
        for candidate in candidate_terms:
            score = 0.0

            # Score 1: Position relative similaire
            candidate_pos = translated_text.find(candidate)
            if candidate_pos != -1:
                candidate_rel_pos = candidate_pos / len(translated_text)
                pos_diff = abs(original_rel_pos - candidate_rel_pos)
                score += (1.0 - pos_diff) * 2.0  # Poids 2x

            # Score 2: Longueur similaire
            len_ratio: float = min(len(original_term), len(candidate)) / max(
                len(original_term), len(candidate)
            )
            score += len_ratio

            # Score 3: Premi√®re lettre similaire
            if original_term[0].lower() == candidate[0].lower():
                score += 0.5

            scores.append((candidate, score))

        # Retourner le meilleur match (seuil minimum 1.0)
        best = max(scores, key=lambda x: x[1])
        return best[0] if best[1] >= 1.0 else None

    # =========================================================================
    # Export et validation
    # =========================================================================

    def export_for_prompt(
        self, max_terms: int = 50, min_confidence: float = 0.5
    ) -> str:
        """
        Exporte le glossaire au format texte pour injection dans un prompt.

        Format: "term1 ‚Üí translation1, term2 ‚Üí translation2, ..."

        Args:
            max_terms: Nombre maximum de termes √† exporter (d√©faut: 50)
            min_confidence: Seuil de confiance minimum (d√©faut: 0.5)

        Returns:
            String format√© pour injection dans le prompt

        Example:
            >>> print(glossary.export_for_prompt())
            'Matrix ‚Üí Matrice, Dr. Sakamoto ‚Üí Dr Sakamoto, DNA ‚Üí ADN'
        """
        terms: list[str] = []

        # Trier par fr√©quence (termes les plus utilis√©s en premier)
        for source_term in sorted(
            self._glossary.keys(),
            key=lambda t: sum(self._glossary[t].values()),
            reverse=True,
        ):
            translation = self.get_translation(
                source_term, min_confidence=min_confidence
            )
            if translation:
                terms.append(f"{source_term} ‚Üí {translation}")

            if len(terms) >= max_terms:
                break

        return ", ".join(terms)

    def get_conflicts(self) -> dict[str, list[str]]:
        """
        Identifie les termes avec des traductions conflictuelles.

        Retourne les termes qui ont plusieurs traductions fr√©quentes
        (aucune ne domine √† >70%).

        Returns:
            Dictionnaire {terme: [traductions_conflictuelles]}

        Example:
            >>> conflicts = glossary.get_conflicts()
            >>> # {'Matrix': ['Matrice', 'Syst√®me']} si traductions √©quilibr√©es
        """
        conflicts: dict[str, list[str]] = {}

        for source_term, translations in self._glossary.items():
            # Ignorer si d√©j√† valid√©
            if source_term in self._validated:
                continue

            # Besoin d'au moins 2 traductions
            if len(translations) < 2:
                continue

            # Calculer les ratios
            total = sum(translations.values())
            max_frequency = max(translations.values())
            dominant_ratio = max_frequency / total

            # Si aucune traduction ne domine √† >70%, c'est un conflit
            if dominant_ratio < 0.7:
                conflicts[source_term] = sorted(
                    translations.keys(),
                    key=lambda t: translations[t],
                    reverse=True,
                )

        return conflicts

    def get_high_confidence_terms(self, min_confidence: float = 0.8) -> dict[str, str]:
        """
        R√©cup√®re les termes avec haute confiance.

        Args:
            min_confidence: Seuil de confiance (d√©faut: 0.8)

        Returns:
            Dictionnaire {terme_source: traduction}

        Example:
            >>> high_conf = glossary.get_high_confidence_terms(min_confidence=0.9)
            >>> # Seulement les termes traduits de mani√®re tr√®s coh√©rente
        """
        result = {}
        for source_term in self._glossary:
            translation = self.get_translation(
                source_term, min_confidence=min_confidence
            )
            if translation:
                result[source_term] = translation
        return result

    def to_dict(self) -> dict[str, str]:
        """
        Exporte le glossaire sous forme de dictionnaire simple.

        Returns:
            Dictionnaire {terme_source: traduction_recommand√©e}

        Example:
            >>> glossary_dict = glossary.to_dict()
            >>> # {'Matrix': 'Matrice', 'DNA': 'ADN', ...}
        """
        result: dict[str, str] = {}

        # Ajouter les traductions valid√©es
        result.update(self._validated)

        # Ajouter les traductions apprises (si pas d√©j√† valid√©es)
        for source_term in self._glossary:
            if source_term not in result:
                translation = self.get_translation(source_term)
                if translation:
                    result[source_term] = translation

        return result

    # =========================================================================
    # Statistiques et utilitaires
    # =========================================================================

    def get_term_count(self) -> int:
        """
        Retourne le nombre de termes dans le glossaire.

        Returns:
            Nombre de termes uniques
        """
        return len(self._glossary)

    def get_statistics(self) -> dict[str, int]:
        """
        Retourne des statistiques sur le glossaire.

        Returns:
            Dictionnaire avec les stats

        Example:
            >>> stats = glossary.get_statistics()
            >>> print(f"Termes: {stats['total_terms']}, Conflits: {stats['conflicting_terms']}")
        """
        return {
            "total_terms": len(self._glossary),
            "validated_terms": len(self._validated),
            "conflicting_terms": len(self.get_conflicts()),
            "unique_translations": sum(
                len(translations) for translations in self._glossary.values()
            ),
        }

    # =========================================================================
    # Nettoyage du glossaire
    # =========================================================================

    def clean_stopwords(self) -> int:
        """
        Retire les mots grammaticaux du glossaire existant.

        Utilise should_exclude_from_glossary() pour identifier les termes
        √† supprimer (articles, pronoms, pr√©positions, etc.).

        Returns:
            Nombre de termes supprim√©s

        Example:
            >>> removed_count = glossary.clean_stopwords()
            >>> print(f"{removed_count} stopwords supprim√©s")
        """
        from .glossary_filters import should_exclude_from_glossary

        terms_to_remove = [
            term
            for term in self._glossary.keys()
            if should_exclude_from_glossary(term)
        ]

        for term in terms_to_remove:
            del self._glossary[term]
            # Supprimer aussi des validations si pr√©sent
            if term in self._validated:
                del self._validated[term]

        return len(terms_to_remove)

    def remove_low_confidence_terms(self, min_occurrences: int = 2) -> int:
        """
        Retire les termes avec tr√®s peu d'occurrences (probables erreurs d'extraction).

        Args:
            min_occurrences: Nombre minimum d'occurrences total (d√©faut: 2)

        Returns:
            Nombre de termes supprim√©s

        Example:
            >>> removed_count = glossary.remove_low_confidence_terms(min_occurrences=2)
            >>> print(f"{removed_count} termes √† faible confiance supprim√©s")
        """
        terms_to_remove = []

        for source_term, translations in self._glossary.items():
            total_occurrences = sum(translations.values())
            if total_occurrences < min_occurrences:
                terms_to_remove.append(source_term)

        for term in terms_to_remove:
            del self._glossary[term]
            # Supprimer aussi des validations si pr√©sent
            if term in self._validated:
                del self._validated[term]

        return len(terms_to_remove)

    def clean_all(
        self, min_occurrences: int = 2, verbose: bool = True
    ) -> dict[str, int]:
        """
        Nettoie le glossaire en appliquant tous les filtres.

        Applique dans l'ordre :
        1. Suppression des stopwords grammaticaux
        2. Suppression des termes √† faible confiance

        Args:
            min_occurrences: Seuil minimum pour remove_low_confidence_terms (d√©faut: 2)
            verbose: Affiche les statistiques de nettoyage (d√©faut: True)

        Returns:
            Dictionnaire avec le nombre de suppressions par cat√©gorie

        Example:
            >>> stats = glossary.clean_all()
            >>> # {'stopwords': 123, 'low_confidence': 45, 'total': 168}
        """
        from .logger import get_logger

        logger = get_logger(__name__)

        if verbose:
            stats_before = self.get_statistics()
            logger.info("üßπ Nettoyage du glossaire...")
            logger.info(f"  Avant : {stats_before['total_terms']} termes")

        # √âtape 1 : Stopwords
        stopwords_removed = self.clean_stopwords()
        if verbose:
            logger.info(f"  Stopwords supprim√©s : {stopwords_removed}")

        # √âtape 2 : Faible confiance
        low_conf_removed = self.remove_low_confidence_terms(min_occurrences)
        if verbose:
            logger.info(f"  Faible confiance supprim√©s : {low_conf_removed}")

        if verbose:
            stats_after = self.get_statistics()
            total_removed = stopwords_removed + low_conf_removed
            logger.info(f"  Apr√®s : {stats_after['total_terms']} termes")
            logger.info(f"‚úÖ Total supprim√© : {total_removed} termes")

        return {
            "stopwords": stopwords_removed,
            "low_confidence": low_conf_removed,
            "total": stopwords_removed + low_conf_removed,
        }

    # =========================================================================
    # Persistance
    # =========================================================================

    def save(self, path: Optional[Path] = None) -> None:
        """
        Sauvegarde le glossaire sur disque.

        Args:
            path: Chemin de sauvegarde (utilise cache_path si non fourni)

        Raises:
            ValueError: Si aucun chemin fourni
        """
        save_path = path or self.cache_path
        if not save_path:
            raise ValueError("No save path provided")

        save_path.parent.mkdir(parents=True, exist_ok=True)

        data = {
            "glossary": {
                source: dict(translations)
                for source, translations in self._glossary.items()
            },
            "validated": self._validated,
        }

        with open(save_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def _load_from_cache(self) -> None:
        """Charge le glossaire depuis le cache."""
        if not self.cache_path or not self.cache_path.exists():
            return

        try:
            with open(self.cache_path, "r", encoding="utf-8") as f:
                data = json.load(f)

            # Reconstruire les defaultdicts
            for source, translations in data.get("glossary", {}).items():
                for trans, count in translations.items():
                    self._glossary[source][trans] = count

            self._validated = data.get("validated", {})

        except (json.JSONDecodeError, KeyError, OSError) as e:
            # En cas d'erreur, ignorer et repartir √† z√©ro
            print(f"‚ö†Ô∏è Erreur lors du chargement du glossaire: {e}")
            self._glossary.clear()
            self._validated.clear()

    def __repr__(self) -> str:
        """Repr√©sentation pour le debug."""
        stats = self.get_statistics()
        return (
            f"Glossary(\n"
            f"  termes: {stats['total_terms']}\n"
            f"  valid√©s: {stats['validated_terms']}\n"
            f"  conflits: {stats['conflicting_terms']}\n"
            f"  haute confiance (>0.8): {len(self.get_high_confidence_terms())}\n"
            f")"
        )

    def __str__(self) -> str:
        """Repr√©sentation textuelle du glossaire."""
        return self.__repr__()
